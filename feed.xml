<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jim-shaw-bluenote.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://jim-shaw-bluenote.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-12T17:07:14+00:00</updated><id>https://jim-shaw-bluenote.github.io//feed.xml</id><title type="html">blank</title><subtitle>Jim Shaw&apos;s academic website. </subtitle><entry><title type="html">Thoughts on metagenomic profiling. Part 1 - as a tool developer</title><link href="https://jim-shaw-bluenote.github.io//blog/2023/profiling-development/" rel="alternate" type="text/html" title="Thoughts on metagenomic profiling. Part 1 - as a tool developer"/><published>2023-11-24T00:00:00+00:00</published><updated>2023-11-24T00:00:00+00:00</updated><id>https://jim-shaw-bluenote.github.io//blog/2023/profiling-development</id><content type="html" xml:base="https://jim-shaw-bluenote.github.io//blog/2023/profiling-development/"><![CDATA[<h3 id="background---i-spent-a-few-months-developing-a-metagenomic-profiler">Background - I spent a few months developing a metagenomic profiler</h3> <p>I recently spent a few months developing a new metagenomic profiler (shotgun, not 16S) called <a href="https://github.com/bluenote-1577/sylph">sylph</a>. I wanted to write down a few opinions on some difficulties during the development process.</p> <p>The post is a combination of</p> <ul> <li>Opinions for future profiler development choices</li> <li>Pre-rebuttal to reviewers :) (just kidding, somewhat)</li> </ul> <p>To be honest, I’m not sure who my target audience is for this blog post. Tool developers may find it interesting. I think practitioners may also find the information useful for illuminating why profilers are the way they are. Hopefully, you’ll at least learn something about taxonomic profiling.</p> <p><em>Note: I am a computational biologist with some opinions. In particular, I’m not a microbiologist nor a definitive source. I’m open to feedback and opinions, especially if I say something wrong about your tool.</em></p> <h2 id="problem-1---ncbi-resource-usability-for-metagenomic-databases">Problem 1 - NCBI resource usability for metagenomic databases</h2> <p>Here is my understanding of some fundamental aspects of (prokaryotic) databases.</p> <p><strong>RefSeq</strong>: a collection of curated genomes from the NCBI. RefSeq encompasses all kingdoms of life and is comprehensive.</p> <p><strong>NCBI Taxonomy</strong>: an official taxonomy from the NCBI. Provides nomenclature (names of taxa) and their hierarchical relationships.</p> <p><strong>Genome Taxonomy Database (GTDB)</strong>: a database + taxonomy for prokaryotes, distinct from RefSeq/NCBI taxonomy.</p> <p>The default database offered by many profilers, such as <a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0">Kraken</a>, uses the NCBI taxonomy + reference genomes from RefSeq. Other methods like <a href="https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-022-01410-z">mOTUs</a> or <a href="https://www.nature.com/articles/s41587-023-01688-w">MetaPhlAn</a> use the NCBI taxonomy as well, but not just RefSeq genomes. The most popular standardized benchmark, <strong>the <a href="https://www.nature.com/articles/s41592-022-01431-4">CAMI</a> benchmark</strong>, uses RefSeq/NCBI taxonomy.</p> <p>I first want to recognize the amazing work done by the creators of the RefSeq and the NCBI taxonomies. They are invaluable resources. However, I have a few grievances <em>from a purely metagenomic profiling perspective</em>.</p> <h3 id="lack-of-refseq-database-standardization-across-tools">Lack of RefSeq database standardization across tools</h3> <p>There is no actual “the RefSeq profiling database”. There are options: are genomes <em>complete</em> or <em>scaffold-level</em>? Are prokaryotic genomes <em>representative or reference</em>? Should viral/eukaryotes be included?</p> <p>The lack of standardization is hampered by RefSeq having a continuous release structure as well as a discrete release structure. In many situations, the continuous release is used. For example, the CAMI datasets use a January 8, 2019 RefSeq+Taxonomy snapshot. This is a problem for benchmarking across different versions (discussed later).</p> <h3 id="parsing-ncbi-taxonomy-is-rife-with-pitfalls">Parsing NCBI taxonomy is rife with pitfalls</h3> <p>This is a major grievance I have. The taxonomy that the NCBI provides is <em>difficult to parse</em>. Here are the basic steps:</p> <ol> <li>There is a serialized tree data structure with internal node IDs called “taxid”s called nodes.dmp. You have to parse this tree data structure.</li> <li>You have to get a mapping from a sequence accession to a taxid called accession2taxid. <a href="https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/">But there are many versions</a>. This then involves parsing a &gt; 1GB gzipped line-delimited file.</li> <li>You get the name for the taxid from a file called names.dmp.</li> </ol> <p>The documentation for parsing taxonomy files, getting accession2taxid, etc are stored mostly in READMEs in an FTP server; I found it intimidating. Ultimately, the NCBI taxonomy is meant for all NCBI sequences, <em>not just RefSeq</em>. This makes it comprehensive and an invaluable, complete resource. But you end up mostly parsing non-useful information if you use RefSeq (there are other options too such as NCBI nt).</p> <p><strong>Why should a user care?</strong> Have you ever tried to add new taxonomic nodes to a Kraken database? <a href="https://github.com/DerrickWood/kraken2/issues/436">Not so forgiving</a>. This isn’t the Kraken developers’ fault, it’s because they had to make it concordant with the NCBI taxonomy, which was never really meant to be used for creating profiling databases.</p> <h2 id="problem-2---benchmarking-against-other-tools-and-taxonomies-is-hard">Problem 2 - Benchmarking against other tools and taxonomies is hard</h2> <p>A large part of developing a profiler comes down to benchmarking (anywhere from 20-50% of the time). I feel that benchmarking profilers is extremely difficult due to <em>database choice</em>. This will be a constant theme for the following points.</p> <h3 id="poor-synthetic-metagenome-construction-is-easy-and-causes-distrust">Poor synthetic metagenome construction is easy and causes distrust</h3> <p>I’ve seen numerous studies that have constructed synthetic metagenomes and claimed results that don’t seem to match real metagenomes. Some common pitfalls:</p> <ol> <li>uniform abundance distributions</li> <li>genomes in the metagenome are <em>the same</em> as genomes in the chosen databases</li> <li>all genomes in the metagenome have a similar genome in the database.</li> </ol> <p>Some benchmark studies claim near-perfect accuracy for many tools, whereas other benchmarks claim most tools perform poorly. This is due to many factors: parameters, database choice, construction of metagenome, etc.</p> <p>For me, this has created distrust of custom synthetic benchmarks. I find myself glazing over custom benchmarks and just looking at CAMI results… but this is not a good thing for the field. While standardized datasets are great, they must lack <em>some</em> aspect of a real metagenome. Also, methods will inevitably overfit to them.</p> <h3 id="how-do-you-choose-a-database-when-comparing-methods">How do you choose a database when comparing methods?</h3> <p>mOTUs and MetaPhlAn use the NCBI taxonomy, but they use a fixed version since their database is not customizable. mOTUs3 <a href="https://github.com/motu-tool/mOTUs/issues/85">seems to use the 8 January 2019</a> for concordance with CAMI, probably, and I believe MetaPhlAn4 <a href="https://forum.biobakery.org/t/which-ncbi-taxdump-version-used-for-metaphlan4-database/4989">depends on the exact version used</a>, but not the 8 Jan 2019 version.</p> <p>When you are comparing tools, there are three options.</p> <ol> <li><strong>(A)</strong> You choose a default database for each tool and compare across databases.</li> <li><strong>(B)</strong> You use <em>the same</em> database+taxonomy for every method.</li> <li><strong>(C)</strong> You use the the same <em>taxonomy</em> but different databases.</li> </ol> <p>Option <strong>(B)</strong> seems mostly fair and is done by many manuscripts.</p> <p>Option <strong>(A), (C)</strong> possibly represents a more accurate representation of actual usage – I bet folks out there use Kraken2’s RefSeq default database and MetaPhlAn4’s default marker database, which are completely different.</p> <p>But <strong>(A)</strong> is an issue due to the following:</p> <ul> <li>One method could use a more comprehensive database or simply choose genomes that are more similar to the metagenome. Which profiler is really better?</li> <li><strong>Taxa names can change between different databases/taxonomy versions.</strong> Comparisons become concerning across taxonomy versions.</li> </ul> <p>Option <strong>(C)</strong> suffers from the first problem above, but not the second.</p> <h3 id="benchmarking-becomes-a-headache">Benchmarking becomes a headache</h3> <p>What’s a good solution? To be honest, I don’t know. Some thoughts:</p> <ul> <li>Marker gene methods are extremely performant. Reviewers will complain if you don’t benchmark, but benchmarking against them means using their fixed taxonomies and databases.</li> <li>Many people are using GTDB now (see <a href="https://www.biorxiv.org/content/10.1101/712166v1">here</a>, for example). I won’t get into NCBI vs GTDB here.</li> <li>I tried to map MetaPhlAn4 taxonomic profiles to the GTDB, but NCBI and GTDB are inherently not 1-to-1, so issues arose.</li> <li>Even if you use the “NCBI taxonomy”, are you comparing across versions?</li> </ul> <p>I am now cautious when I interpret results between MetaPhlAn/mOTUs and non-marker gene methods. Many of the studies that I’ve read seem to lack information about how cross-taxonomy results were obtained. The study <a href="https://www.frontiersin.org/articles/10.3389/fmicb.2021.643682/full">here</a> by Parks et al. is an exception and did a really great job of outlining its benchmarking procedure.</p> <h2 id="problem-3----standardizing-taxonomic-profiling-outputs">Problem 3 - Standardizing taxonomic profiling outputs</h2> <p>Profilers output their profiles in different formats, and it’s up to the user to standardize them. This is annoying for users, but also for developers. I’ll discuss why I think this situation ended up happening.</p> <h3 id="methods-do-different-things">Methods do different things</h3> <ol> <li> <p><strong>Taxonomic profiling</strong> is the quantification of taxa for a metagenomic sample. Taxonomic profiling, of course, requires a taxonomy.</p> </li> <li> <p>I will define <strong>genome profiling</strong> (or metagenome profiling) as the quantification of the genomes in a database. Note that genome profiling does not require a taxonomy, but genome profiles can be turned into a taxonomic profile by incorporating a taxonomy.</p> </li> <li> <p>Some methods are <strong>sequence classifiers</strong>. They classify each sequence (i.e. read) against a database and then use this for either taxonomic or genome quantification. The term <strong>taxonomic binner</strong> is used; I dislike the term because it gets confused with contig binning.</p> </li> </ol> <p>Sequence classifiers, genome profilers, and taxonomic profilers are not exclusive; most methods do multiple things. As such, they will output different things.</p> <p>For example, Kraken does (1) and (3). Sylph does (2) and can incorporate taxonomic information downstream for (1). MetaPhlAn4 does (1), but algorithmically it’s more like (2) – it <a href="https://forum.biobakery.org/t/origin-clade-specific-marker-genes/3806">aligns reads to a species-level database of marker genes</a> (the “genomes”) and then sums up abundances.</p> <h3 id="we-need-a-standardized-taxonomic-profiling-output">We need a standardized taxonomic profiling output</h3> <p>Standardizing sequence classifiers and genome profiling is out of scope, but we can at least attempt to standardize <strong>taxonomic profiling</strong> outputs. The <a href="https://github.com/CAMI-challenge/contest_information/blob/master/file_formats/CAMI_TP_specification.mkd">CAMI format for taxonomic profiling</a> is a possible choice, but it is not the standard for Kraken or MetaPhlAn. MetaPhlAn has its own output, which is similar.</p> <p>I think these formats are reasonable, but here are some rudimentary thoughts.</p> <ul> <li>I don’t like taxids included in a format specification. This requires a relationship to the NCBI taxonomy. They could be included as additional tags.</li> <li>Existing output formats do not allow for customization. The <a href="https://samtools.github.io/hts-specs/SAMv1.pdf">SAM format</a> allows for optional tags for each record to record additional information, for example.</li> <li>We should really be attaching some sort of confidence score to each classification. Think about how useful MAPQs are in read mapping.</li> <li>We should record the exact taxonomies (i.e. versions) used. Taxonomic profiles and taxonomy are intertwined.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Developing profilers is hard. The differences between methods on (1) database, (2) taxonomy, and (3) outputs make development and benchmarking difficult. I have a newfound respect for the people behind large standardized benchmarks, especially the folks behind CAMI for tackling these challenges.</p> <p>I think correctly handling all of the pitfalls in benchmarking and development is too challenging for someone who wants to create better algorithms.</p> <p>My ideal future as a tool developer?</p> <ol> <li>Let’s have a collection of standardized and versioned databases, each with (1) a set of genomes and (2) an associated taxonomy file that is easy to parse.</li> <li>We can extend each database by simply dropping in a fasta file and adding a new line in the taxonomy file.</li> <li>Let’s allow each tool to use one of these standardized databases and output standardized taxonomic profiles, making benchmarking and cross-profile comparison easy. This allows practitioners to try different databases easily too.</li> </ol> <h3 id="example-taxonomy-format-solution">Example taxonomy format solution</h3> <p>Here is a simple format: store a correspondence between each genome file/sequence and a string in a two-column TSV file:</p> <p>Genome_file Tax_string</p> <p>GCA_945889495.1.fasta d__Bacteria;p__Desulfobacterota_B;…;g__DP-20;s__DP-20 sp945889495</p> <p>GCA_934500585.1.fasta d__Bacteria;p__Bacillota_A;…;g__RQCD01;s__RQCD01 sp008668455</p> <p>That’s it – just one file. This would allow you to reconstruct a taxonomic tree, giving you <em>the exact same information</em> as the NCBI files.</p> <p>This would allow your favourite MAG classification tool (GTDB-TK, for example) to be integrated into a taxonomic profiler database. To add a new species, you just add a genome and its new taxonomy string to the file. This is roughly, but not approximately, <a href="https://github.com/bluenote-1577/sylph/wiki/Integrating-taxonomic-information-with-sylph">what sylph does</a>.</p> <p><strong>Acknowledgements:</strong> Thanks to the people in my <a href="https://bsky.app/profile/jimshaw.bsky.social/post/3ka5ftbdgbt2f">bluesky complaint thread</a> who gave their thoughts on profilers.</p> <p><strong>Complaints/Thoughts/Inaccuracies?:</strong> Feel free to e-mail me or DM me at my twitter @jim_elevator.</p>]]></content><author><name></name></author><category term="jekyll"/><category term="update"/><summary type="html"><![CDATA[Background - I spent a few months developing a metagenomic profiler]]></summary></entry><entry><title type="html">A proof sketch of seed-chain-extend runtime being close to O(m log n)</title><link href="https://jim-shaw-bluenote.github.io//blog/2022/almost-mlogn-bound-proof-sketch/" rel="alternate" type="text/html" title="A proof sketch of seed-chain-extend runtime being close to O(m log n)"/><published>2022-10-31T00:00:00+00:00</published><updated>2022-10-31T00:00:00+00:00</updated><id>https://jim-shaw-bluenote.github.io//blog/2022/almost-mlogn-bound-proof-sketch</id><content type="html" xml:base="https://jim-shaw-bluenote.github.io//blog/2022/almost-mlogn-bound-proof-sketch/"><![CDATA[<p>In our (me and <a href="https://yunwilliamyu.net/content/">Yun William Yu’s</a>) new paper <a href="https://www.biorxiv.org/content/10.1101/2022.10.14.512303v1">“Seed-chain-extend alignment is accurate and runs in close to O(m log n) time for similar sequences: a rigorous average-case analysis”</a>, we give rigorous bounds on seed-chain-extend alignment using average-case analysis on random, mutating strings. This blog post is meant to be:</p> <ol> <li>A high-level exposition of the main ideas of the paper for people somewhat familiar with k-mers, alignment, and chaining.</li> <li>An intuitive proof sketch of the runtime being close to \(O(m \log n).\)</li> </ol> <h3 id="main-motivation">Main motivation</h3> <p>Optimal alignment in the theoretical worse case is difficult; it takes \(O(mn)\) time for sequences of length \(n\) and \(m\) where \(m &lt; n\) using e.g. Needleman-Wunsch or Smith-Waterman. Real aligners don’t just align reads or two genomes together using an \(O(mn)\) algorithm but use heuristics instead.</p> <p>A popular heuristic used for alignment by aligners like minimap2 is k-mer seed-chain-extend, which uses k-mer seeds to approximately find an alignment. I’ll assume familiarity with what a chain of k-mer anchors is from now on. If you’re not familiar, I highly recommend the recent review article <a href="https://www.biorxiv.org/content/10.1101/2022.05.21.492932v2">“A survey of mapping algorithms in the long-reads era”</a>.</p> <p>However, seed-chain-extend is still \(O(mn)\) in the worst-case: consider two strings \(S = AAAAA...\) and \(S' = AAAAA....\) of length \(n\) and \(m\); there will be \(O(mn)\) k-mer matches in this case, so finding all anchors already takes \(O(mn)\) time. Furthermore, there is no guarantee that the resulting alignment is optimal (although the <em>chain</em> may be optimal). In practice, however, the runtime is much better than \(O(mn)\) and the alignment usually works out okay.</p> <p>Therefore, in order to break through the \(O(mn)\) runtime barrier, we use average-case analysis instead. This means we take the expected runtime over all possible inputs under some probabilistic model on our inputs. The idea is that as long as the inputs are not of the form \(AAAAA...\) <em>too often</em> under our random model, we can still do better than \(O(mn)\) on average.</p> <h3 id="random-input-model">Random input model</h3> <p>To do average-case analysis, we need a random model on our inputs to take an expectation over. When we align sequences, we generally believe there is some similarity between them, so aligning two random strings isn’t the correct model. We instead use an independent substitution model. We let \(S\) be a random string, and \(S'\) be a mutated substring of \(S\) where we take a substring of \(S\) and then mutate each character to a different letter with probability \(\theta\). The length of \(S\) is \(\sim n\) and \(S'\) is \(\sim m\) where \(\sim\) hides factors of \(k\) lurking around; see Section 2 for clarification.</p> <p>We don’t model indels, but such independent substitution models have been used before to model k-mer statistics relatively well (e.g. mash). In my opinion, the much bigger issue is that repeats aren’t correctly modeled; if someone wants to take a stab at modeling random mutating string models with repeats, let me know!</p> <h3 id="what-did-we-prove">What did we prove?</h3> <p>There are three main things we prove in the paper. Remember, \(m &lt; n\) and \(\theta\) is the mutation rate of our string.</p> <ol> <li>Expected runtime of extension is \(O(m n^{f(\theta)} \log n)\) for some \(f(\theta)\). It turns out \(f(\theta) &lt; 0.08\) when \(\theta = 0.05\); see Supplementary Figure 7 for explicit \(f(\theta)\). This dominates the overall runtime.</li> <li>Expected runtime of chaining is \(O(m \log m)\) (actually slightly better; see Theorem 1 in our paper).</li> <li>Expected accuracy of resulting alignment: we can recover more than \(1 - O(1/\sqrt m)\) fraction of the “homologous” bases that are related.</li> </ol> <p>In summary, we prove that the expected runtime is much better than \(O(mn)\) and actually give a result on alignment accuracy, which I don’t think has been done before. We also prove some things about sketched versions with k-mer subsampling, but I won’t discuss that here.</p> <p>The chaining result (2) involves messing around with statistics of k-mers and that there are not too many k-mer hits. It’s an argument on mutating substring matching with a few technical lemmas. The accuracy result (3) is more messy to define and prove, and a lot of effort was dedicated to this. Both involve a mix of probabilistic and combinatorial tools. I won’t touch on what it means to be able to recover homologous bases here; see Section 4 if you’re interested.</p> <p>I think the extension result (1) is the most interesting, and the main idea behind the proof is relatively simple. I’ll spend the rest of the blog talking about this result.</p> <h3 id="core-assumptions-on-seed-chain-extend">Core assumptions on seed-chain-extend</h3> <p>I will spend the rest of this blog explaining the main idea behind result (1) above.</p> <p>In Section 2 of our paper, we go through our exact model seed-chain-extend. I’ll list the most important points below.</p> <ol> <li>We let \(k = C \log n\) for some constant \(2 &lt; C &lt; 3\) which depends on only \(\theta\), and \(n\) is the size of the reference. So \(k\), the k-mer length, is increasing as the genome size increases. \(C\) is defined in Theorem 1 in our paper.</li> <li>We use fixed length k-mer seeds. Anchors are exact matches of k-mers. Assume no sketching in this blog post.</li> <li>We use a linear gap cost and solve it in \(O(N \log N)\) time where \(N\) is the number of anchors. See for example <a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02168-z">minigraph</a> for the definition of linear gap cost and the \(O(N \log N)\) range-min/max-query solution. This isn’t important for the rest of my post, but the linear gap cost is used in the paper to prove some of the assumptions I list below. In principle, any gap cost that penalizes large gaps should work, but you can’t ignore gaps (i.e. don’t just take the chain with most anchors).</li> <li>k-mer anchors are <em>allowed to overlap</em>. If you’re experienced with chaining, you know that allowing overlaps without explicitly accounting for overlaps in the cost may not model the chain properly. For us, however, it is mathematically convenient to allow overlaps.</li> <li>We perform <em>quadratic time extension</em> between gaps in the resulting chain. We don’t actually care about the cost (e.g. edit, affine, dual-affine, …) for this.</li> </ol> <h3 id="how-do-we-prove-extension-is-fast">How do we prove extension is fast?</h3> <p>Intuitively, we just want to show that the gaps between anchors in the resulting chain are small and not plentiful. Remember that the resulting chain is now <em>random</em> since our inputs are random. The optimal chain can be kind of weird, so we look at the longest <em>homologous</em> chain, which I define below.</p> <p>In the following image, vertical bases are derived from the same ancestral sequence, and grey bases are mutated. The blue k-mer anchors come from positions that are hence homologous, whereas the red anchors are spurious anchors resulting from randomness. The alignment matrix in the bottom panel shows how k-mer matches look when considering a dynamic programming matrix.</p> <p><img src="/assets/img/anchors_and_chains_example.png" alt="test" width="60%"/></p> <p>We can always obtain a chain by just taking all blue, homologous anchors. Let’s call this <em>the</em> homologous chain.</p> <p><strong>Claim</strong>: The homologous chain may not be optimal, but in our paper, we prove that the runtime through the <em>possibly non-optimal</em> homologous chain is the dominant big O term.</p> <p>The above claim takes quite a bit of work, but I think it’s relatively believable. Intuitively, we pick \(k = C \log n\) big enough such that spurious anchors don’t happen too often. The homologous chain is much, much easier to work with than an arbitrary optimal chain, so our problem becomes much easier now.</p> <h3 id="modeling-gaps-between-homologous-k-mers">Modeling gaps between homologous k-mers</h3> <p>The following sections are a high-level version of Appendix D.5 in our paper.</p> <p>In the above figure, we have a gap labeled \(G_1\). These are also called “islands”, and the number of islands that appear in a random mutating sequence is studied in <a href="https://www.liebertpub.com/doi/10.1089/cmb.2021.0431">“The Statistics of k-mers from a Sequence Undergoing a Simple Mutation Process Without Spurious Matches</a>. Our problem is slightly different since we want the length of the islands, and not just the number of islands.</p> <p>Assuming we have gaps \(G_1, G_2, ...\), the runtime of extension is \(O(G_1^2) + O(G_2^2) + ...\) because it takes quadratic time to extend through gaps. Alternatively, the dynamic programming matrix induced by the gap of size \(G_1\) is of size \(G_1^2\). However, the \(G_i\)s are random variables, so we want \(O(\mathbb{E} [G_1^2]) + O(\mathbb{E}[G_2^2]) + ...\) The number of \(G_i\)s is also random variable, so calculating this sum directly is challenging.</p> <p><strong>Definition</strong>: To model gaps properly, let us instead define a random variable \(Y_i\) where \(Y_i = \ell &gt; 0\) if</p> <ol> <li>the k-mer starting at position \(i\) is unmutated (none of its bases are mutated)</li> <li>and the k-mer starting at position \(i + \ell + k\) is unmutated</li> <li>and no k-mers between these two flanking k-mers are unmutated (i.e. they’re all mutated).</li> </ol> <p>If any of these conditions fail, \(Y_i = 0\).</p> <p>It’s not too hard to see that \(Y_i\) represents the size of a gap starting at position \(i+k\) if and only if a gap of size \(&gt;0\) exists, and is \(0\) otherwise. In particular, the \(G_i\)s are in 1-to-1 correspondence with non-zero \(Y_i\)s, so</p> \[Y_1^2 + Y_2^2 + ... + Y_m^2 = G_1^2 + G_2^2 + ...\] <p>We sum up to \(m\) because there are \(m\) homologous k-mers since \(m &lt; n\). But now, the <em>number</em> of \(Y_i\)s is no longer a random variable! So we can just take expectations and use linearity of expectation to get a result.</p> <h3 id="making-k-mers-independent-as-an-upper-bound">Making k-mers independent as an upper bound.</h3> <p>We’re almost there, but not quite yet. We do a trick to upper bound \(Y_1^2 + ... + Y_m^2\). The problem is that in conditions (1), (2), and (3) above, the k-mers are not independent. Precisely, the “k-mers between these two flanking k-mers” in (3) share bases with the flanking k-mers, so they’re dependent under our mutation model.</p> <p>To remove this dependence, we instead <strong>only consider k-mers that are spaced exactly k bases apart</strong>. That is, let \(K\) be a maximal set of k-mers spaced exactly \(k\) bases apart from each other. k-mers in \(K\) do not overlap at all. The bottom set of k-mers in the below picture is \(K\).</p> <p style="text-align: center;"><img src="/assets/img/K_kmer.png" alt="test" width="30%"/></p> <p><strong>Definition</strong>: Now let \(Y_i^K\) be the random variable such that \(Y_i^K = x\) if</p> <ol> <li>the k-mer starting at position \(i\) is unmutated (none of its bases are mutated) <strong>and this k-mer is in \(K\)</strong></li> <li>and the k-mer starting at position \(i + x + k\) is unmutated <strong>and this k-mer is in \(K\)</strong></li> <li>and no k-mers <strong>in \(K\)</strong> between these two flanking k-mers are unmutated (i.e. they’re all mutated).</li> </ol> <p>And we let \(Y_i^K = 0\) if any of these conditions are violated. \(Y_i^K\)s represent the gaps when only considering k-mers in \(K\).</p> <p><strong>Claim</strong>: It should be intuitively obvious that the gaps are <em>larger</em> when only considering k-mers in \(K\) instead of all k-mers, thus we get an <em>upper bound</em> by considering \(Y_i^K\)s; we prove this more rigorously in Appendix D.5.</p> <p>Thus we are left with bounding</p> \[\text{Runtime} \leq O(\mathbb{E}[Y_1^2 + ... + Y_m^2]) \leq O(\mathbb{E}[(Y_1^K)^2 + ... + (Y_m^K)^2])\] <p>Recall that \(m\) is the size of \(S'\) (modulo factors of \(k\), which are small), the smaller string. Now we can see that all except for about \(m/k\) of the \(Y_i^K\) are always \(0\) since there are only \(m/k\) k-mers in \(K\), so only \(m/k\) of the random variables satisfy condition (1) above.</p> <h3 id="calculating-expectation-of-y_ik2">Calculating expectation of \((Y_i^K)^2\).</h3> <p>Let’s compute \(\Pr(Y_i^K = x)\). I claim that assuming the k-mer at \(i\) is in \(K\),</p> \[\Pr(Y_i^K = k \cdot \ell) \leq (1-\theta)^k \cdot (1-\theta)^k \cdot (1 - (1-\theta)^k)^{\ell}\] <p>Firstly, \(Y_i^K\) can not equal non-multiples of \(k\) by our construction. The first \((1-\theta)^k\) comes from condition (1) above, and the second follows from condition (2). The probability a k-mer is unmutated is \((1-\theta)^k\) because all mutations are independent. The last term comes from the fact that if there are \(\ell \cdot k\) bases between the first and last k-mer, there are exactly \(\ell\) k-mers in between them (in \(K\)). These k-mers are independent, so the probability that all of them are mutated is \((1 - (1-\theta)^k)^\ell\). The \(\leq\) follows because the gap size can not be greater than \(m\), so the actual value would be 0 in that case.</p> <p>Now we can finally calculate what we wanted, which is \(\mathbb{E}[(Y_i^K)^2]\). This is</p> \[\mathbb{E}[(Y_i^K)^2] = \sum_{x=1}^\infty x^2 \Pr(Y_i^K = x) \leq \sum_{\ell = 1}^\infty (\ell k)^2 (1-\theta)^{2k} (1 - (1-\theta)^k)^{\ell}.\] <p>We only sum over \(\ell\), so this is</p> \[= (1-\theta)^{2k} k^2 \sum_{\ell = 1}^\infty \ell^2 (1-(1-\theta)^k)^{\ell}.\] <p>Geometric series calculus (or Mathematica) tells us that \(\sum_{i=1}^\infty i^2 x^i = \frac{x (x+1)}{(1-x)^3}\) (there’s a typo in version 1 of our paper on pg. 28; this is the correct formula). Thus plugging this in, we get</p> \[\mathbb{E}[(Y_i^K)^2] \leq (1-\theta)^{2k} k^2 \cdot O \bigg( \frac{1}{(1-\theta)^{3k}} \bigg) = O \bigg( \frac{k^2}{(1-\theta)^k} \bigg)\] <p>after remembering that \((1-\theta)^k = o(1)\) because we assumed that \(k = C \log n\) way back for some constant \(C\). Now recall there are \(m/k\) non-zero \(Y_i^K\) random variables, so our final number of gaps squared is</p> \[\mathbb{E}[(Y_1^K)^2] + ... + \mathbb{E}[(Y_m^K)^2] \leq \frac{m}{k} \cdot O \bigg( \frac{k^2}{(1-\theta)^k} \bigg) = O \bigg( m k (1-\theta)^{-k}\bigg).\] <p>After a final substitution of \(k = C \log n\), we get that</p> \[= O \bigg (m \log n (1 - \theta)^{-C \log n} \bigg) = O\bigg( m \log n n^{-C \log (1 - \theta)}\bigg)\] <p>where \(-C \log (1 - \theta) = f (\theta)\), and we’re done. In the paper, we take \(C \sim \frac{2}{1 + 2 \log(1-\theta)}\) for reasons relating to the accuracy result, so \(C\) actually depends on \(\theta\) as well.</p> <h3 id="conclusion">Conclusion</h3> <p>We showed that the runtime of extending through the gaps in the homologous chain of k-mers is \(O(m n^{f(\theta)} \log n )\), which is almost \(O(m \log n)\) when \(\theta\) is small. For example, we can explicit calculate \(f(0.05) &lt; 0.08\) when \(\theta\) is \(0.05\), so the runtime in this case this is \(O(m n^{0.08} \log n)\). \(n^{0.08}\) is actually smaller than \(\log n\) when \(n &lt; 10^{21}\), so it’s really, really small for realistic \(n\).</p> <p>Remember, I did not claim that we’re actually aligning through the homologous chain; we’re aligning through the optimal chain given by linear gap cost chaining. However, in the paper, we show that the actual runtime is dominated by this homologous chain extension runtime. This shouldn’t be too head-scratching; most of the time (especially without repeats), we expect the correct chain to be the obvious, homologous one. It’s surprisingly challenging to prove, though, and comes down to bounding <em>breaks</em>, which I won’t go into here.</p> <p>Last thought I’d like to mention: one of the punchlines of our paper involves sketching. We show that <em>this same bound holds</em> even when we subsample the k-mers. The proof for that result follows the same intuition as this proof, but it actually uses a non-independent Chernoff bound to bound the \(Y_i\)s instead, leading to some slightly different techniques.</p>]]></content><author><name></name></author><category term="jekyll"/><category term="update"/><summary type="html"><![CDATA[In our (me and Yun William Yu’s) new paper “Seed-chain-extend alignment is accurate and runs in close to O(m log n) time for similar sequences: a rigorous average-case analysis”, we give rigorous bounds on seed-chain-extend alignment using average-case analysis on random, mutating strings. This blog post is meant to be:]]></summary></entry><entry><title type="html">Average contig length times 1.6783469… is equal to N50</title><link href="https://jim-shaw-bluenote.github.io//blog/2022/average-contig-length-n50/" rel="alternate" type="text/html" title="Average contig length times 1.6783469… is equal to N50"/><published>2022-09-26T00:00:00+00:00</published><updated>2022-09-26T00:00:00+00:00</updated><id>https://jim-shaw-bluenote.github.io//blog/2022/average-contig-length-n50</id><content type="html" xml:base="https://jim-shaw-bluenote.github.io//blog/2022/average-contig-length-n50/"><![CDATA[<h3 id="probabilistic-setup-of-sequence-assembly-outputs">Probabilistic setup of sequence assembly outputs</h3> <p>When one analyzes sequence assemblies, a mysterious definition they inevitably encounter is the N50. N50 is commonly defined as the size \(L\) of the smallest contig such that summing all contigs of size \(\geq L\) gives at least half of the total sequence assembly length. The reason for using N50 intuitively is that we don’t want small contigs, of which there may be many, to skew the average contig length. See <a href="http://www.acgt.me/blog/2013/7/8/why-is-n50-used-as-an-assembly-metric.html">here</a> or the <a href="https://en.wikipedia.org/wiki/N50,_L50,_and_related_statistics">Wikipedia article</a> for more info.</p> <p>Given a set of contigs coming from an assembly, how does the N50 relate to the average contig length? This may seem like a silly question because they aren’t related in general; you can easily come up with distributions of contig sizes with fixed average length and wildly varying N50s. However, I’ll show that under some (maybe not so mild) assumptions, the average contig length and N50 are related by a surprisingly simple formula.</p> <p>Let’s make the following assumptions:</p> <ol> <li> <p>Assume that we have a fixed number of \(n\) contigs of size \(x_1,...,x_n\) and define \(\sum_{i=1}^n x_i = N\). Here \(x_1,..,x_n\) are the sizes of contigs output by an assembly, and \(N\) is our putative assembly length.</p> </li> <li> <p>Assume that \(x_i\) are random, i.i.d, and exponentially distributed \(x_i \sim Exp(\frac{1}{\lambda})\) with mean length \(\lambda.\) Contigs have continuous, random size in our setup.</p> </li> </ol> <p>I chose an exponential distribution because contig lengths tend to have a heavy tail with a few large contigs and many small ones.</p> <p>Under our probabilistic setup, the average length of the contigs is \(\mathbb{E}[x_i] = \lambda\), but <em>can we calculate what the N50 is analytically?</em></p> <h3 id="defining-n50-probabilistically">Defining N50 probabilistically</h3> <p>First, let’s define the <em>empirical N50</em> rigorously.</p> <p><em>Definition</em>: The empirical N50, \(\hat{N_{50}}\), is a random variable defined as the smallest \(x_j = \hat{N_{50}}\) where \(x_j \in \{x_1,...,x_n\}\) such that</p> \[\sum_{x_i \geq \hat{N_{50}}} x_i = \sum_{i=1}^n x_i \mathbb{1}_{x_i \geq \hat{N_{50}}} \geq \frac{N}{2}.\] <p>This is just a mathematical generalization of the original definition of the N50 if you have a bunch of samples (i.e. contigs). Now I’ll introduce a probabilistic definition of the N50.</p> <p><em>Definition</em>: The N50 is a constant \(N_{50}\) satisfying</p> \[\mathbb{E}[\sum_{i=1}^n x_i \mathbb{1}_{x_i \geq N_{50}}] = \frac{\mathbb{E}[N]}{2} = \frac{n\lambda}{2}.\] <p>The N50 is not a random variable, but a specific constant defined to satisfy an equation. Intuitively, the N50 is what we believe the value of empirical N50, \(\hat{N_{50}}\), should be when \(n\), the number of samples, gets really large. Notice that this is essentially a generalization of the probabilistic definition of the median and the empirical median. Here is a suggestive sketch of an argument that these two quantities are related.</p> <p>Notice that</p> \[\frac{N}{2} + \max_{m=1,...,n} x_m \geq \sum_{x_i \geq \hat{N_{50}}} x_i = \sum_{i=1}^n x_i \mathbb{1}_{x_i \geq \hat{N_{50}}} \geq \frac{N}{2}\] <p>follows after thinking about the definition of the empirical N50 for a bit. It turns that \(\mathbb{E}[\max_{m=1,...,n} x_m] = \sum_{i=1}^n \frac{1}{i} = O(\log n)\) is the nth harmonic number, see <a href="https://stats.stackexchange.com/questions/324274/how-to-find-the-expectation-of-the-maximum-of-independent-exponential-variables">this explanation</a>. Then taking expectations where \(\mathbb{E}[N] = n \lambda\) and dividing by \(n\), we get that</p> \[\frac{\lambda}{2} + o(1) &gt; \frac{\mathbb{E}[\sum_{i=1}^n X_i \mathbb{1}_{x_i \geq N_{50}}]}{n} \geq \frac{\lambda}{2}.\] <p>So \(\frac{\mathbb{E}[\sum_{x_i \geq \hat{N_{50}}} x_i]}{n} \rightarrow \lambda/2.\) Thus, from the definition of empirical N50,</p> \[\lim_{n \rightarrow \infty} \frac{\mathbb{E}[\sum_{i=1}^n x_i \mathbb{1}_{x_i \geq \hat{N_{50}}}]}{n} = \frac{\mathbb{E}[\sum_{i=1}^n x_i \mathbb{1}_{x_i \geq N_{50}}]}{n}.\] <p>This is the reason behind the definition, and suggests that \(\hat{N_{50}} \rightarrow N_{50}\) is some sense, probably in probability.</p> <p>I haven’t tried too hard to prove that \(\hat{N_{50}} \rightarrow N_{50}\) as n gets large. The proof should probably be similar to the <a href="https://stats.stackexchange.com/questions/72023/convergence-in-probability-of-empirical-median">proof that the empirical median converges to the true median</a>. In my simulations, it seems to be the case that \(\mathbb{E}[\hat{N_{50}}] \rightarrow N_{50}\), so let’s just assume that these quantities are related for the rest of the post. Maybe someone else can take a stab at it; let me know how it goes!</p> <h3 id="calculating-the-n50">Calculating the N50</h3> <p>Let’s define \(\sigma = N_{50}\) for the rest of the post, for ease of notation. We can actually calculate the N50 as follows. Because \(\sigma\) is a constant, \(x_i \mathbb{1}_{x_i \geq \sigma}\) are identically distributed. Thus to find the N50, we just have to solve the equation</p> \[\mathbb{E}[\sum_{i=1}^n x_i \mathbb{1}_{x_i \geq \sigma}] = n \int_{z \geq \sigma}^\infty z \frac{1}{\lambda} e^{-\frac{z}{\lambda}} dz = \frac{n \lambda }{2}\] <p>for \(\sigma\). The \(n\) cancels out nicely, and evaluating the integral yields the equation</p> \[(\sigma + \lambda) = \frac{\lambda}{2} e^{\frac{\sigma}{\lambda}}\] <p>This is actually not a trivial equation to solve. You can try isolating for \(\sigma\) and you’ll see that it doesn’t work so easily. Here’s how you solve it:</p> <p>First, we make the substitution \(y = \sigma + \lambda\). This simplifies it to</p> \[y = \frac{\lambda}{2} e^{\frac{y}{\lambda} - 1}\] <p>and after another round of simplifications this becomes</p> \[2e = \frac{\lambda}{y}e^{\frac{y}{\lambda}} \iff \frac{1}{2e} = \frac{y}{\lambda}e^{-\frac{y}{\lambda}}\] <p>assuming \(y \not = 0\), of course. Finally, we let \(y \mapsto -y\) to get</p> \[\frac{-1}{2e} = \frac{y}{\lambda}e^{\frac{y}{\lambda}}.\] <p>At this point, you’re probably either wondering about why we went through this seemingly random derivation, or you can see exactly what the next step is. We make one last substituion \(w = \frac{y}{\lambda}\) to get</p> \[\frac{-1}{2e} = w e^{w}.\] <p>This equation still isn’t easy to solve, but luckily, this type of equation pops ups all the time and has been standardized. The \(w\) that gives the solution is defined as the output of the <a href="https://en.wikipedia.org/wiki/Lambert_W_function">Lambert W function</a>, \(W(z)\), evaluated at \(z = \frac{-1}{2e}\). \(W(z)\) is a multivalued function when \(z\) is complex, and for \(z\) real, it has two solutions when \(\frac{-1}{e} \leq z &lt; 0\), as is our case. To make \(\sigma &gt; 0\), we need the function evaluated on the branch defined as \(W_{-1}(z)\). It turns out that \(W_{-1}(\frac{-1}{2e}) = -2.6783469...\). Remembering that \(w = \frac{-y}{\lambda} = \frac{-\sigma}{\lambda} - 1\), we get that finally</p> \[\sigma / \lambda = 1.6783469...\] <p><strong>So \(\sigma\), the N50, is a constant multiple of the average contig length!</strong></p> <p>It seems that this value \(1.6783469...\) appears elsewhere in statistical theory. A quick google search yields the article “Asymptotic Inversion of Incomplete Gamma Functions” by N.M Temme (1992) as an example where this value appears.</p> <p>So under our exponential model, along with our numerous assumptions, the N50 and the average contig length is always related just by a constant multiple, no matter what the average length is.</p> <h3 id="does-this-actually-work-in-practice">Does this actually work in practice?</h3> <p>To apply this theory, we can’t look at genomes that have been meticulously refined, as they no longer have exponentially distributed contig lengths.</p> <p>To investigate a more raw dataset, I took a set of MAGs, i.e. metagenome assembled genomes from the study <a href="https://www.nature.com/articles/s41564-017-0012-7">here</a> and uploaded <a href="http://enve-omics.ce.gatech.edu/data/fastani">here</a>. I calculated the N50s for a bunch of the genome assemblies, and here is the resulting histogram.</p> <p><img src="/assets/img/empirical_n50_avg_len_ratio.png" alt="Actual N50/Average length distribution"/></p> <p>It turns out that the predicted N50/Average length ratio is about 3.4 percent off from the empirical value. Considering how often and how badly theoretical models fail in bioinformatics, I’ll consider this a win :).</p>]]></content><author><name></name></author><category term="jekyll"/><category term="update"/><summary type="html"><![CDATA[Probabilistic setup of sequence assembly outputs]]></summary></entry></feed>