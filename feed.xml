<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jim-shaw-bluenote.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://jim-shaw-bluenote.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-16T02:05:58+00:00</updated><id>https://jim-shaw-bluenote.github.io//feed.xml</id><title type="html">blank</title><subtitle>Jim Shaw&apos;s academic website. </subtitle><entry><title type="html">Developing sylph - a look into bioinformatics tool development</title><link href="https://jim-shaw-bluenote.github.io//blog/2024/developing-sylph/" rel="alternate" type="text/html" title="Developing sylph - a look into bioinformatics tool development"/><published>2024-11-18T00:00:00+00:00</published><updated>2024-11-18T00:00:00+00:00</updated><id>https://jim-shaw-bluenote.github.io//blog/2024/developing-sylph</id><content type="html" xml:base="https://jim-shaw-bluenote.github.io//blog/2024/developing-sylph/"><![CDATA[<p>In this blog post, I will talk about the process of developing <a href="https://github.com/bluenote-1577/sylph">sylph</a>, a bioinformatics tool we (me and my advisor, <a href="https://yunwilliamyu.net/content/">William</a>) developed. Sylph is now published in <a href="https://www.nature.com/articles/s41587-024-02412-y">Nature Biotechnology</a>.</p> <p>I’ve always found the inner workings of research interesting. This is my contribution. My goal is to demystify tool development in bioinformatics a bit.</p> <h4 id="prelude-what-is-sylph">Prelude: what is sylph?</h4> <p>Sylph is a new method/software that detects what organisms are present in a <a href="https://www.nature.com/articles/nbt.3935">metagenomic DNA sequencing</a> sample of a microbiome. This is sometimes called taxonomic or metagenomic <em>profiling</em>.</p> <p>This post won’t be about the sylph algorithm, but about its development process. I’ll assume some familiarity with computational metagenomics going forward, but it’s not strictly necessary.</p> <h4 id="part-1---initial-motivation-lets-write-a-faster-mash-late-2022">Part 1 - Initial motivation: let’s write a faster Mash (late 2022)</h4> <hr/> <p>The first paper I ever read in the field of bioinformatics was the <a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0997-x">Mash</a> paper by Ondov et al. back in 2017 (during an undergraduate internship with <a href="https://www.a-star.edu.sg/gis/our-people/faculty-staff/members/niranjan-nagarajan">Niranjan</a> in Singapore).</p> <p>If you’re unfamiliar with Mash, it’s simply a method for calculating the similarity between two genomes. It does so by analyzing <em>sketches</em>, or small subsets, of k-mers. See <a href="https://genomeinformatics.github.io/mash-screen/">this related blog post</a> for more background. <a href="https://github.com/sourmash-bio/sourmash">Sourmash</a> is a related tool; I recall meeting with <a href="http://ivory.idyll.org/lab/">Titus Brown</a> in 2022 and learned quite a bit about sourmash from him.</p> <p>The Mash algorithm is simple but powerful. It has tremendously influenced how I feel bioinformatics tools should be: <strong>fast, easy to use, and accurate enough for generating biological hypotheses</strong>.</p> <p><strong>Sylph started from a simple curiosity:</strong> when writing <a href="https://www.nature.com/articles/s41592-023-02018-3">a genome-to-genome calculation tool</a> in 2022, I noticed that I could create k-mer sketches faster than Mash. This was done by adapting the k-mer processing routines from <a href="https://github.com/lh3/minimap2">minimap2</a> and learning how to use SIMD instructions to speed stuff up.</p> <p>I thought it would be fun to write a stripped-down, faster k-mer sketching tool for personal use—I didn’t think about publishing at this point. This was built relatively quickly while I was zoning out at a mathematics conference that I attended back home in Vancouver.</p> <h4 id="part-2---stumbling-across-skmer-from-sarmashghi-et-al-mid-late-2022">Part 2 - Stumbling across Skmer from Sarmashghi et al. (mid-late 2022)</h4> <hr/> <p>Prior to even starting sylph, I was writing a paper about <a href="https://genome.cshlp.org/content/early/2023/03/29/gr.277637.122">sequence alignment theory</a> in mid-2022. I wanted to justify the usage of a particular statistical model of k-mer statistics. I went to <a href="https://doi.org/10.1089/cmb.2021.0431">a familiar paper on k-mer statistics</a> by Blanca et al. to see what they cited, finding the paper <a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1632-4">“Skmer: assembly-free and alignment-free sample identification using genome skims”</a> by Sarmashghi et al.</p> <p>I remember skimming through this paper, and while I didn’t read through the math at the time, I understood the basic idea: <strong>low-coverage sequencing creates problems for k-mer sketching</strong>.</p> <p>Back to late-2022, I ran into the following problem while writing my new tool:</p> <h5 id="the-low-abundance-problem-technical-can-skip">The low-abundance problem (technical; can skip)</h5> <hr/> <p>Suppose a genome (in some database) <strong>only shares a small fraction of its k-mers within a metagenome sequencing sample</strong>. This could</p> <ul> <li>be due to low sequencing depth in the metagenome (k-mers are not sequenced)</li> </ul> <p><strong>OR</strong></p> <ul> <li>be due to spurious k-mer matches that arise from a different species.</li> </ul> <p>I wanted to compute the <strong>containment average nucleotide identity</strong> (ANI) by using k-mers: this containment ANI generalizes genome-genome ANI to <em>genome-metagenome</em> ANI. If you want to do this using k-mers, sequencing depth becomes an issue.</p> <hr/> <p>It turns out that the aforementioned Skmer paper <em>almost</em> tackled the exact problem above, but there was an issue: the Skmer paper only compares genomes-to-genomes, but I needed to compare genomes-to-metagenomes.</p> <p>So during 2022/2023, I spent a few nights at my local McDonald’s, reading through the Skmer paper and working out the rough math for sylph. I implemented the statistical correction method and it seemed to work on some simulated sequences – success!</p> <h4 id="part-3---not-knowing-what-to-do-for-5-months-2022-dec---2023-apr">Part 3 - Not knowing what to do for 5 months (2022 Dec - 2023 Apr)</h4> <hr/> <p>The statistical model for sylph was completed in early 2023, but I had no idea what to do with sylph. <strong>I hadn’t figured out how to make it a taxonomic profiler yet.</strong> Let me explain the problem:</p> <h5 id="the-abundance-estimation-problem-technical-can-skip">The abundance estimation problem (technical; can skip)</h5> <hr/> <p>If you want the abundance of some organism in your metagenomic sample, most profilers do two steps: (1) classify reads against reference genomes and then (2) count the proportion of reads assigned to each genome.</p> <p>However, if you have (1) two genomes from the same genus, e.g. <em>Escherichia</em>, in your database and (2) a single E. coli read, the read may be similar to multiple <em>Escherichia</em> genomes. How do you “classify” reads in this case? All profilers have some way of dealing with this issue (e.g., Kraken uses a taxonomy).</p> <p>I didn’t have a good way of dealing with this issue (even for “dereplicated” species-level databases) because sylph doesn’t map reads—it just checks for k-mers, of which many can be shared between different species.</p> <hr/> <p><strong>Back to sylph</strong>: In summary, I had a method that can only check if a genome is contained in a sample (quickly and more accurately than other methods), but <strong>not compute the abundance of the organism</strong> (i.e., do <em>profiling</em>).</p> <p>Funnily enough, I didn’t try hard to tackle this issue. I thought that this was not the “right” problem to solve for sylph. During this time, I had a few other ideas that didn’t pan out. Pulling up an old notebook, I see words such as “k-mer EM algorithm strain resolution” or “optimized sub-linear k-mer indexing”…</p> <h4 id="part-4---try-to-build-around-an-incomplete-algorithm-2023-apr---2023-aug">Part 4 - Try to build around an incomplete algorithm (2023 Apr - 2023 Aug)</h4> <hr/> <p>Eventually, in April/May of 2023, I settled on just presenting sylph, but <strong>without the ability to estimate the abundances of organisms</strong>. And by July/August 2023, I wrote a brief communication (~3 pages) based on sylph’s statistical model and showed it could be used for quickly detecting low-coverage genomes.</p> <p>While preparing for manuscript submission, we worked a lot on figuring out how to present the paper. This was rather difficult given that people often only care about abundances.</p> <h4 id="part-5---profiling-winner-take-all-heuristic-saves-the-day-2023-aug">Part 5 - Profiling: winner-take-all heuristic saves the day? (2023 Aug)</h4> <hr/> <p>I was planning on submitting sylph in August 2023, and we even had the cover letter ready. However, I was randomly browsing through the sourmash/Mash GitHub one day (I don’t even remember what for), and I came across the “<strong>winner-take-all</strong>” k-mer heuristic. This is explained below:</p> <h5 id="reassigning-k-mers-and-abundance-technical-can-skip">Reassigning k-mers and abundance (technical, can skip)</h5> <hr/> <p>The idea behind the winner-take-all heuristic is simple: if a k-mer is shared across many genomes in the database, assign it to the genome with the highest estimated containment ANI. This could solve the abundance problem: if E. coli and E. fergusonii are in your database but only E. coli is present, all of the k-mers for E. fergusonii should be assigned to E. coli. E. fergusonii will have no k-mers left and be thresholded out (see Figure 1 in the sylph paper).</p> <p>To the best of my knowledge, this heuristic is mentioned online (since 2017) but not explicitly explained anywhere, which is why I didn’t run across it. I probably <em>should</em> have thought of this idea. <a href="https://dib-lab.github.io/2020-paper-sourmash-gather/">Irber et al.</a> use a very similar idea that I was familiar with, but the winner-take-all approach made more sense with the statistical model.</p> <hr/> <p><strong>Trying the heuristic out:</strong> I didn’t believe this simple heuristic would work, but I decided to give it a try. It took less than an hour to implement, and it worked surprisingly well, so I hit pause on the submission. Comparatively, the statistical model is a more interesting and important contribution, but this was the breakthrough… surprisingly.</p> <p>I finally had the capabilities to estimate organism abundance in August 2023, but I <em>did not</em> want to do the benchmarking necessary for a paper. Benchmarking is much less fun than developing new methods. Unfortunately, the project seemed to have potential.</p> <p>In September 2023, I went to Tokyo to visit <a href="https://sites.google.com/site/mcfrith/">Martin Frith</a> for three months. During these three months, all sylph-related work consisted of benchmarking sylph’s new profiling abilities, creating figures, and rewriting the paper. Recall that it took only 1 hour to implement the winner-take-all heuristic…</p> <p>Luckily, sylph worked well. <em>Why</em> does it work well? Perhaps this is the subject of a talk or another post. Anyways, by late November 2023, I started applying for academic jobs. The benchmarks seemed okay; the paper was tight enough, so we decided to get sylph out as a preprint.</p> <h4 id="part-6---bad-results-on-real-reads-2023-dec">Part 6 - Bad results on real reads? (2023 Dec)</h4> <hr/> <p>In December 2023, Florian Plaza Oñate came to me with <a href="https://github.com/bluenote-1577/sylph/issues/5">a bug report</a>, suggesting that sylph was not performing well on real data sets. This was a head-scratching result.</p> <p>As any bioinformatics tool developer knows, your algorithms always perform worse on real data than benchmark data. There will be <em>some</em> aspect of someone’s data that violates your method’s assumptions. However, the data Florian showed me seemed fishy enough that it wasn’t just due to a small biological violation of the model… it seemed systemetically off.</p> <p>A key component of sylph’s model is a <em>stochastic uniformity assumption</em> for read sequencing, leading to Poisson statistics for sequencing. After thinking for a while, I knew this had to be the issue. However, it was not clear if this was an inherent issue with the model, or a technological artefact.</p> <p>An unwritten law in bioinformatics is that if you don’t know what’s happening, you visualize it in the <a href="https://igv.org/doc/desktop/">IGV</a>. So I manually inspected some read alignments, and I found way more duplicated sequences than I expected. It turned out that <strong>PCR duplicates were messing up sylph’s statistical model, violating the Poisson assumption</strong>. Did you know that many Illumina sequencing runs can have &gt; 30% of reads being PCR duplicates? Perhaps embarassingly, I did not.</p> <p>After discovering this issue, I came up with a simple locality-sensitive hashing algorithm for removing PCR duplicates. This seemed to work okay, but I had to rerun a few of my results. This led to an important update and a preprint revision; the revision was also motivated due to people being mad at me for not benchmarking against the latest version of MetaPhlAn :) (lesson learned). This was completed in January 2024.</p> <h4 id="concluding-thoughts-2024-now">Concluding thoughts (2024, now)</h4> <hr/> <p>I’m pleasantly surprised at sylph’s reception so far. I’ve had people tell me personally/on social media that they’ve found sylph useful. Regardless of the journal we got into, as a tool developer, I’ve already been very happy with the outcome.</p> <p>Recently, I’m thrilled that sylph has been used for profiling <strong>~2 million sequencing runs</strong> in <a href="https://www.biorxiv.org/content/10.1101/2024.03.08.584059v1">Hunt et al.</a>. Also, apparently Oxford Nanopore Technologies found that <a href="https://a.storyblok.com/f/196663/x/3cff79e4ec/microbeconference_20240502-1333.pdf">sylph was the best method on their long reads</a> as well (see Fig. 3).</p> <h4 id="concluding-opinions-thoughts-and-notes-on-development">Concluding opinions, thoughts, and notes on development</h4> <hr/> <p><strong>Development time usage</strong>: Not including paper writing, I estimate I spent 75% of the time on experiments, plotting, and benchmarking compared to 25% developing the method (i.e. writing code, refining the algorithm). I wrote more snakemake, python, and jupyter notebook code than actual sylph code. Why did it take so long?</p> <ol> <li>Figuring out <strong>how</strong> to benchmark is a pain for taxonomic/metagenomic profilers. You have to simulate your own benchmarks, make scripts for normalizing outputs, wrangle other methods’ databases, etc. There are many many potential pitfalls.</li> <li>Figuring out <strong>what</strong> to benchmark is a challenge. This requires a lot of thought.</li> </ol> <p>These time proportions depend on the project. For comparison, sylph’s algorithm is simpler than a genome assembler, which would take more development time. I also think that it’s okay to not spend all your time on software development; I’m a scientist, not a software engineer. A significant part of a tool development project <em>should</em> be figuring out how to benchmark and fit your method into scientific context.</p> <p><strong>Do people care about benchmarks anyway?</strong> Here’s another thing I noticed: many people don’t trust bioinformatics benchmarks. Lots of people fall into two categories:</p> <ul> <li>If a tool’s performance is not critical, the author benchmarks don’t matter that much. People will just play around with your software.</li> <li>If a tool’s performance is critical, you’ll do your own tests instead of trusting author benchmarks; benchmarks from the tool’s authors always place their tool at the top anyways (mysteriously).</li> </ul> <p><strong>Algorithm progress is highly non-linear as a function of time.</strong> It’s hard to predict progress. Furthermore, this project started out way different than how it ended up. I think a more natural approach to method development is correct, rather than shooting for a goal. This means you’re tackling problems in the correct way.</p> <p><strong>Benchmarking taxonomic profilers is hard</strong>. I already <a href="https://jim-shaw-bluenote.github.io/blog/2023/profiling-development/">wrote a post about benchmarking taxonomic profilers</a>. Taxonomic profilers are especially bad because database construction is not intuitive and time consuming.</p> <p><strong>The end</strong>: hopefully this was informative. Feel free to let me know if you have any complaints.</p>]]></content><author><name></name></author><category term="jekyll"/><category term="update"/><summary type="html"><![CDATA[In this blog post, I will talk about the process of developing sylph, a bioinformatics tool we (me and my advisor, William) developed. Sylph is now published in Nature Biotechnology.]]></summary></entry><entry><title type="html">Thoughts on metagenomic profiling. Part 1 - as a tool developer</title><link href="https://jim-shaw-bluenote.github.io//blog/2023/profiling-development/" rel="alternate" type="text/html" title="Thoughts on metagenomic profiling. Part 1 - as a tool developer"/><published>2023-11-24T00:00:00+00:00</published><updated>2023-11-24T00:00:00+00:00</updated><id>https://jim-shaw-bluenote.github.io//blog/2023/profiling-development</id><content type="html" xml:base="https://jim-shaw-bluenote.github.io//blog/2023/profiling-development/"><![CDATA[<h3 id="background---i-spent-a-few-months-developing-a-metagenomic-profiler">Background - I spent a few months developing a metagenomic profiler</h3> <p>I recently spent a few months developing a new metagenomic profiler (shotgun, not 16S) called <a href="https://github.com/bluenote-1577/sylph">sylph</a>. I wanted to write down a few opinions on some difficulties during the development process.</p> <p>The post is a combination of</p> <ul> <li>Opinions for future profiler development choices</li> <li>Pre-rebuttal to reviewers :) (just kidding, somewhat)</li> </ul> <p>To be honest, I’m not sure who my target audience is for this blog post. Tool developers may find it interesting. I think practitioners may also find the information useful for illuminating why profilers are the way they are. Hopefully, you’ll at least learn something about taxonomic profiling.</p> <p><em>Note: I am a computational biologist with some opinions. In particular, I’m not a microbiologist nor a definitive source. I’m open to feedback and opinions, especially if I say something wrong about your tool.</em></p> <h2 id="problem-1---ncbi-resource-usability-for-metagenomic-databases">Problem 1 - NCBI resource usability for metagenomic databases</h2> <p>Here is my understanding of some fundamental aspects of (prokaryotic) databases.</p> <p><strong>RefSeq</strong>: a collection of curated genomes from the NCBI. RefSeq encompasses all kingdoms of life and is comprehensive.</p> <p><strong>NCBI Taxonomy</strong>: an official taxonomy from the NCBI. Provides nomenclature (names of taxa) and their hierarchical relationships.</p> <p><strong>Genome Taxonomy Database (GTDB)</strong>: a database + taxonomy for prokaryotes, distinct from RefSeq/NCBI taxonomy.</p> <p>The default database offered by many profilers, such as <a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0">Kraken</a>, uses the NCBI taxonomy + reference genomes from RefSeq. Other methods like <a href="https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-022-01410-z">mOTUs</a> or <a href="https://www.nature.com/articles/s41587-023-01688-w">MetaPhlAn</a> use the NCBI taxonomy as well, but not just RefSeq genomes. The most popular standardized benchmark, <strong>the <a href="https://www.nature.com/articles/s41592-022-01431-4">CAMI</a> benchmark</strong>, uses RefSeq/NCBI taxonomy.</p> <p>I first want to recognize the amazing work done by the creators of the RefSeq and the NCBI taxonomies. They are invaluable resources. However, I have a few grievances <em>from a purely metagenomic profiling perspective</em>.</p> <h3 id="lack-of-refseq-database-standardization-across-tools">Lack of RefSeq database standardization across tools</h3> <p>There is no actual “the RefSeq profiling database”. There are options: are genomes <em>complete</em> or <em>scaffold-level</em>? Are prokaryotic genomes <em>representative or reference</em>? Should viral/eukaryotes be included?</p> <p>The lack of standardization is hampered by RefSeq having a continuous release structure as well as a discrete release structure. In many situations, the continuous release is used. For example, the CAMI datasets use a January 8, 2019 RefSeq+Taxonomy snapshot. This is a problem for benchmarking across different versions (discussed later).</p> <h3 id="parsing-ncbi-taxonomy-is-rife-with-pitfalls">Parsing NCBI taxonomy is rife with pitfalls</h3> <p>This is a major grievance I have. The taxonomy that the NCBI provides is <em>difficult to parse</em>. Here are the basic steps:</p> <ol> <li>There is a serialized tree data structure with internal node IDs called “taxid”s called nodes.dmp. You have to parse this tree data structure.</li> <li>You have to get a mapping from a sequence accession to a taxid called accession2taxid. <a href="https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/">But there are many versions</a>. This then involves parsing a &gt; 1GB gzipped line-delimited file.</li> <li>You get the name for the taxid from a file called names.dmp.</li> </ol> <p>The documentation for parsing taxonomy files, getting accession2taxid, etc are stored mostly in READMEs in an FTP server; I found it intimidating. Ultimately, the NCBI taxonomy is meant for all NCBI sequences, <em>not just RefSeq</em>. This makes it comprehensive and an invaluable, complete resource. But you end up mostly parsing non-useful information if you use RefSeq (there are other options too such as NCBI nt).</p> <p><strong>Why should a user care?</strong> Have you ever tried to add new taxonomic nodes to a Kraken database? <a href="https://github.com/DerrickWood/kraken2/issues/436">Not so forgiving</a>. This isn’t the Kraken developers’ fault, it’s because they had to make it concordant with the NCBI taxonomy, which was never really meant to be used for creating profiling databases.</p> <h2 id="problem-2---benchmarking-against-other-tools-and-taxonomies-is-hard">Problem 2 - Benchmarking against other tools and taxonomies is hard</h2> <p>A large part of developing a profiler comes down to benchmarking (anywhere from 20-50% of the time). I feel that benchmarking profilers is extremely difficult due to <em>database choice</em>. This will be a constant theme for the following points.</p> <h3 id="poor-synthetic-metagenome-construction-is-easy-and-causes-distrust">Poor synthetic metagenome construction is easy and causes distrust</h3> <p>I’ve seen numerous studies that have constructed synthetic metagenomes and claimed results that don’t seem to match real metagenomes. Some common pitfalls:</p> <ol> <li>uniform abundance distributions</li> <li>genomes in the metagenome are <em>the same</em> as genomes in the chosen databases</li> <li>all genomes in the metagenome have a similar genome in the database.</li> </ol> <p>Some benchmark studies claim near-perfect accuracy for many tools, whereas other benchmarks claim most tools perform poorly. This is due to many factors: parameters, database choice, construction of metagenome, etc.</p> <p>For me, this has created distrust of custom synthetic benchmarks. I find myself glazing over custom benchmarks and just looking at CAMI results… but this is not a good thing for the field. While standardized datasets are great, they must lack <em>some</em> aspect of a real metagenome. Also, methods will inevitably overfit to them.</p> <h3 id="how-do-you-choose-a-database-when-comparing-methods">How do you choose a database when comparing methods?</h3> <p>mOTUs and MetaPhlAn use the NCBI taxonomy, but they use a fixed version since their database is not customizable. mOTUs3 <a href="https://github.com/motu-tool/mOTUs/issues/85">seems to use the 8 January 2019</a> for concordance with CAMI, probably, and I believe MetaPhlAn4 <a href="https://forum.biobakery.org/t/which-ncbi-taxdump-version-used-for-metaphlan4-database/4989">depends on the exact version used</a>, but not the 8 Jan 2019 version.</p> <p>When you are comparing tools, there are three options.</p> <ol> <li><strong>(A)</strong> You choose a default database for each tool and compare across databases.</li> <li><strong>(B)</strong> You use <em>the same</em> database+taxonomy for every method.</li> <li><strong>(C)</strong> You use the the same <em>taxonomy</em> but different databases.</li> </ol> <p>Option <strong>(B)</strong> seems mostly fair and is done by many manuscripts.</p> <p>Option <strong>(A), (C)</strong> possibly represents a more accurate representation of actual usage – I bet folks out there use Kraken2’s RefSeq default database and MetaPhlAn4’s default marker database, which are completely different.</p> <p>But <strong>(A)</strong> is an issue due to the following:</p> <ul> <li>One method could use a more comprehensive database or simply choose genomes that are more similar to the metagenome. Which profiler is really better?</li> <li><strong>Taxa names can change between different databases/taxonomy versions.</strong> Comparisons become concerning across taxonomy versions.</li> </ul> <p>Option <strong>(C)</strong> suffers from the first problem above, but not the second.</p> <h3 id="benchmarking-becomes-a-headache">Benchmarking becomes a headache</h3> <p>What’s a good solution? To be honest, I don’t know. Some thoughts:</p> <ul> <li>Marker gene methods are extremely performant. Reviewers will complain if you don’t benchmark, but benchmarking against them means using their fixed taxonomies and databases.</li> <li>Many people are using GTDB now (see <a href="https://www.biorxiv.org/content/10.1101/712166v1">here</a>, for example). I won’t get into NCBI vs GTDB here.</li> <li>I tried to map MetaPhlAn4 taxonomic profiles to the GTDB, but NCBI and GTDB are inherently not 1-to-1, so issues arose.</li> <li>Even if you use the “NCBI taxonomy”, are you comparing across versions?</li> </ul> <p>I am now cautious when I interpret results between MetaPhlAn/mOTUs and non-marker gene methods. Many of the studies that I’ve read seem to lack information about how cross-taxonomy results were obtained. The study <a href="https://www.frontiersin.org/articles/10.3389/fmicb.2021.643682/full">here</a> by Parks et al. is an exception and did a really great job of outlining its benchmarking procedure.</p> <h2 id="problem-3----standardizing-taxonomic-profiling-outputs">Problem 3 - Standardizing taxonomic profiling outputs</h2> <p>Profilers output their profiles in different formats, and it’s up to the user to standardize them. This is annoying for users, but also for developers. I’ll discuss why I think this situation ended up happening.</p> <h3 id="methods-do-different-things">Methods do different things</h3> <ol> <li> <p><strong>Taxonomic profiling</strong> is the quantification of taxa for a metagenomic sample. Taxonomic profiling, of course, requires a taxonomy.</p> </li> <li> <p>I will define <strong>genome profiling</strong> (or metagenome profiling) as the quantification of the genomes in a database. Note that genome profiling does not require a taxonomy, but genome profiles can be turned into a taxonomic profile by incorporating a taxonomy.</p> </li> <li> <p>Some methods are <strong>sequence classifiers</strong>. They classify each sequence (i.e. read) against a database and then use this for either taxonomic or genome quantification. The term <strong>taxonomic binner</strong> is used; I dislike the term because it gets confused with contig binning.</p> </li> </ol> <p>Sequence classifiers, genome profilers, and taxonomic profilers are not exclusive; most methods do multiple things. As such, they will output different things.</p> <p>For example, Kraken does (1) and (3). Sylph does (2) and can incorporate taxonomic information downstream for (1). MetaPhlAn4 does (1), but algorithmically it’s more like (2) – it <a href="https://forum.biobakery.org/t/origin-clade-specific-marker-genes/3806">aligns reads to a species-level database of marker genes</a> (the “genomes”) and then sums up abundances.</p> <h3 id="we-need-a-standardized-taxonomic-profiling-output">We need a standardized taxonomic profiling output</h3> <p>Standardizing sequence classifiers and genome profiling is out of scope, but we can at least attempt to standardize <strong>taxonomic profiling</strong> outputs. The <a href="https://github.com/CAMI-challenge/contest_information/blob/master/file_formats/CAMI_TP_specification.mkd">CAMI format for taxonomic profiling</a> is a possible choice, but it is not the standard for Kraken or MetaPhlAn. MetaPhlAn has its own output, which is similar.</p> <p>I think these formats are reasonable, but here are some rudimentary thoughts.</p> <ul> <li>I don’t like taxids included in a format specification. This requires a relationship to the NCBI taxonomy. They could be included as additional tags.</li> <li>Existing output formats do not allow for customization. The <a href="https://samtools.github.io/hts-specs/SAMv1.pdf">SAM format</a> allows for optional tags for each record to record additional information, for example.</li> <li>We should really be attaching some sort of confidence score to each classification. Think about how useful MAPQs are in read mapping.</li> <li>We should record the exact taxonomies (i.e. versions) used. Taxonomic profiles and taxonomy are intertwined.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Developing profilers is hard. The differences between methods on (1) database, (2) taxonomy, and (3) outputs make development and benchmarking difficult. I have a newfound respect for the people behind large standardized benchmarks, especially the folks behind CAMI for tackling these challenges.</p> <p>I think correctly handling all of the pitfalls in benchmarking and development is too challenging for someone who wants to create better algorithms.</p> <p>My ideal future as a tool developer?</p> <ol> <li>Let’s have a collection of standardized and versioned databases, each with (1) a set of genomes and (2) an associated taxonomy file that is easy to parse.</li> <li>We can extend each database by simply dropping in a fasta file and adding a new line in the taxonomy file.</li> <li>Let’s allow each tool to use one of these standardized databases and output standardized taxonomic profiles, making benchmarking and cross-profile comparison easy. This allows practitioners to try different databases easily too.</li> </ol> <h3 id="example-taxonomy-format-solution">Example taxonomy format solution</h3> <p>Here is a simple format: store a correspondence between each genome file/sequence and a string in a two-column TSV file:</p> <p>Genome_file Tax_string</p> <p>GCA_945889495.1.fasta d__Bacteria;p__Desulfobacterota_B;…;g__DP-20;s__DP-20 sp945889495</p> <p>GCA_934500585.1.fasta d__Bacteria;p__Bacillota_A;…;g__RQCD01;s__RQCD01 sp008668455</p> <p>That’s it – just one file. This would allow you to reconstruct a taxonomic tree, giving you <em>the exact same information</em> as the NCBI files.</p> <p>This would allow your favourite MAG classification tool (GTDB-TK, for example) to be integrated into a taxonomic profiler database. To add a new species, you just add a genome and its new taxonomy string to the file. This is roughly, but not approximately, <a href="https://github.com/bluenote-1577/sylph/wiki/Integrating-taxonomic-information-with-sylph">what sylph does</a>.</p> <p><strong>Acknowledgements:</strong> Thanks to the people in my <a href="https://bsky.app/profile/jimshaw.bsky.social/post/3ka5ftbdgbt2f">bluesky complaint thread</a> who gave their thoughts on profilers.</p> <p><strong>Complaints/Thoughts/Inaccuracies?:</strong> Feel free to e-mail me or DM me at my twitter @jim_elevator.</p>]]></content><author><name></name></author><category term="jekyll"/><category term="update"/><summary type="html"><![CDATA[Background - I spent a few months developing a metagenomic profiler]]></summary></entry><entry><title type="html">A proof sketch of seed-chain-extend runtime being close to O(m log n)</title><link href="https://jim-shaw-bluenote.github.io//blog/2022/almost-mlogn-bound-proof-sketch/" rel="alternate" type="text/html" title="A proof sketch of seed-chain-extend runtime being close to O(m log n)"/><published>2022-10-31T00:00:00+00:00</published><updated>2022-10-31T00:00:00+00:00</updated><id>https://jim-shaw-bluenote.github.io//blog/2022/almost-mlogn-bound-proof-sketch</id><content type="html" xml:base="https://jim-shaw-bluenote.github.io//blog/2022/almost-mlogn-bound-proof-sketch/"><![CDATA[<p>In our (me and <a href="https://yunwilliamyu.net/content/">Yun William Yu’s</a>) new paper <a href="https://www.biorxiv.org/content/10.1101/2022.10.14.512303v1">“Seed-chain-extend alignment is accurate and runs in close to O(m log n) time for similar sequences: a rigorous average-case analysis”</a>, we give rigorous bounds on seed-chain-extend alignment using average-case analysis on random, mutating strings. This blog post is meant to be:</p> <ol> <li>A high-level exposition of the main ideas of the paper for people somewhat familiar with k-mers, alignment, and chaining.</li> <li>An intuitive proof sketch of the runtime being close to \(O(m \log n).\)</li> </ol> <h3 id="main-motivation">Main motivation</h3> <p>Optimal alignment in the theoretical worse case is difficult; it takes \(O(mn)\) time for sequences of length \(n\) and \(m\) where \(m &lt; n\) using e.g. Needleman-Wunsch or Smith-Waterman. Real aligners don’t just align reads or two genomes together using an \(O(mn)\) algorithm but use heuristics instead.</p> <p>A popular heuristic used for alignment by aligners like minimap2 is k-mer seed-chain-extend, which uses k-mer seeds to approximately find an alignment. I’ll assume familiarity with what a chain of k-mer anchors is from now on. If you’re not familiar, I highly recommend the recent review article <a href="https://www.biorxiv.org/content/10.1101/2022.05.21.492932v2">“A survey of mapping algorithms in the long-reads era”</a>.</p> <p>However, seed-chain-extend is still \(O(mn)\) in the worst-case: consider two strings \(S = AAAAA...\) and \(S' = AAAAA....\) of length \(n\) and \(m\); there will be \(O(mn)\) k-mer matches in this case, so finding all anchors already takes \(O(mn)\) time. Furthermore, there is no guarantee that the resulting alignment is optimal (although the <em>chain</em> may be optimal). In practice, however, the runtime is much better than \(O(mn)\) and the alignment usually works out okay.</p> <p>Therefore, in order to break through the \(O(mn)\) runtime barrier, we use average-case analysis instead. This means we take the expected runtime over all possible inputs under some probabilistic model on our inputs. The idea is that as long as the inputs are not of the form \(AAAAA...\) <em>too often</em> under our random model, we can still do better than \(O(mn)\) on average.</p> <h3 id="random-input-model">Random input model</h3> <p>To do average-case analysis, we need a random model on our inputs to take an expectation over. When we align sequences, we generally believe there is some similarity between them, so aligning two random strings isn’t the correct model. We instead use an independent substitution model. We let \(S\) be a random string, and \(S'\) be a mutated substring of \(S\) where we take a substring of \(S\) and then mutate each character to a different letter with probability \(\theta\). The length of \(S\) is \(\sim n\) and \(S'\) is \(\sim m\) where \(\sim\) hides factors of \(k\) lurking around; see Section 2 for clarification.</p> <p>We don’t model indels, but such independent substitution models have been used before to model k-mer statistics relatively well (e.g. mash). In my opinion, the much bigger issue is that repeats aren’t correctly modeled; if someone wants to take a stab at modeling random mutating string models with repeats, let me know!</p> <h3 id="what-did-we-prove">What did we prove?</h3> <p>There are three main things we prove in the paper. Remember, \(m &lt; n\) and \(\theta\) is the mutation rate of our string.</p> <ol> <li>Expected runtime of extension is \(O(m n^{f(\theta)} \log n)\) for some \(f(\theta)\). It turns out \(f(\theta) &lt; 0.08\) when \(\theta = 0.05\); see Supplementary Figure 7 for explicit \(f(\theta)\). This dominates the overall runtime.</li> <li>Expected runtime of chaining is \(O(m \log m)\) (actually slightly better; see Theorem 1 in our paper).</li> <li>Expected accuracy of resulting alignment: we can recover more than \(1 - O(1/\sqrt m)\) fraction of the “homologous” bases that are related.</li> </ol> <p>In summary, we prove that the expected runtime is much better than \(O(mn)\) and actually give a result on alignment accuracy, which I don’t think has been done before. We also prove some things about sketched versions with k-mer subsampling, but I won’t discuss that here.</p> <p>The chaining result (2) involves messing around with statistics of k-mers and that there are not too many k-mer hits. It’s an argument on mutating substring matching with a few technical lemmas. The accuracy result (3) is more messy to define and prove, and a lot of effort was dedicated to this. Both involve a mix of probabilistic and combinatorial tools. I won’t touch on what it means to be able to recover homologous bases here; see Section 4 if you’re interested.</p> <p>I think the extension result (1) is the most interesting, and the main idea behind the proof is relatively simple. I’ll spend the rest of the blog talking about this result.</p> <h3 id="core-assumptions-on-seed-chain-extend">Core assumptions on seed-chain-extend</h3> <p>I will spend the rest of this blog explaining the main idea behind result (1) above.</p> <p>In Section 2 of our paper, we go through our exact model seed-chain-extend. I’ll list the most important points below.</p> <ol> <li>We let \(k = C \log n\) for some constant \(2 &lt; C &lt; 3\) which depends on only \(\theta\), and \(n\) is the size of the reference. So \(k\), the k-mer length, is increasing as the genome size increases. \(C\) is defined in Theorem 1 in our paper.</li> <li>We use fixed length k-mer seeds. Anchors are exact matches of k-mers. Assume no sketching in this blog post.</li> <li>We use a linear gap cost and solve it in \(O(N \log N)\) time where \(N\) is the number of anchors. See for example <a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02168-z">minigraph</a> for the definition of linear gap cost and the \(O(N \log N)\) range-min/max-query solution. This isn’t important for the rest of my post, but the linear gap cost is used in the paper to prove some of the assumptions I list below. In principle, any gap cost that penalizes large gaps should work, but you can’t ignore gaps (i.e. don’t just take the chain with most anchors).</li> <li>k-mer anchors are <em>allowed to overlap</em>. If you’re experienced with chaining, you know that allowing overlaps without explicitly accounting for overlaps in the cost may not model the chain properly. For us, however, it is mathematically convenient to allow overlaps.</li> <li>We perform <em>quadratic time extension</em> between gaps in the resulting chain. We don’t actually care about the cost (e.g. edit, affine, dual-affine, …) for this.</li> </ol> <h3 id="how-do-we-prove-extension-is-fast">How do we prove extension is fast?</h3> <p>Intuitively, we just want to show that the gaps between anchors in the resulting chain are small and not plentiful. Remember that the resulting chain is now <em>random</em> since our inputs are random. The optimal chain can be kind of weird, so we look at the longest <em>homologous</em> chain, which I define below.</p> <p>In the following image, vertical bases are derived from the same ancestral sequence, and grey bases are mutated. The blue k-mer anchors come from positions that are hence homologous, whereas the red anchors are spurious anchors resulting from randomness. The alignment matrix in the bottom panel shows how k-mer matches look when considering a dynamic programming matrix.</p> <p><img src="/assets/img/anchors_and_chains_example.png" alt="test" width="60%"/></p> <p>We can always obtain a chain by just taking all blue, homologous anchors. Let’s call this <em>the</em> homologous chain.</p> <p><strong>Claim</strong>: The homologous chain may not be optimal, but in our paper, we prove that the runtime through the <em>possibly non-optimal</em> homologous chain is the dominant big O term.</p> <p>The above claim takes quite a bit of work, but I think it’s relatively believable. Intuitively, we pick \(k = C \log n\) big enough such that spurious anchors don’t happen too often. The homologous chain is much, much easier to work with than an arbitrary optimal chain, so our problem becomes much easier now.</p> <h3 id="modeling-gaps-between-homologous-k-mers">Modeling gaps between homologous k-mers</h3> <p>The following sections are a high-level version of Appendix D.5 in our paper.</p> <p>In the above figure, we have a gap labeled \(G_1\). These are also called “islands”, and the number of islands that appear in a random mutating sequence is studied in <a href="https://www.liebertpub.com/doi/10.1089/cmb.2021.0431">“The Statistics of k-mers from a Sequence Undergoing a Simple Mutation Process Without Spurious Matches</a>. Our problem is slightly different since we want the length of the islands, and not just the number of islands.</p> <p>Assuming we have gaps \(G_1, G_2, ...\), the runtime of extension is \(O(G_1^2) + O(G_2^2) + ...\) because it takes quadratic time to extend through gaps. Alternatively, the dynamic programming matrix induced by the gap of size \(G_1\) is of size \(G_1^2\). However, the \(G_i\)s are random variables, so we want \(O(\mathbb{E} [G_1^2]) + O(\mathbb{E}[G_2^2]) + ...\) The number of \(G_i\)s is also random variable, so calculating this sum directly is challenging.</p> <p><strong>Definition</strong>: To model gaps properly, let us instead define a random variable \(Y_i\) where \(Y_i = \ell &gt; 0\) if</p> <ol> <li>the k-mer starting at position \(i\) is unmutated (none of its bases are mutated)</li> <li>and the k-mer starting at position \(i + \ell + k\) is unmutated</li> <li>and no k-mers between these two flanking k-mers are unmutated (i.e. they’re all mutated).</li> </ol> <p>If any of these conditions fail, \(Y_i = 0\).</p> <p>It’s not too hard to see that \(Y_i\) represents the size of a gap starting at position \(i+k\) if and only if a gap of size \(&gt;0\) exists, and is \(0\) otherwise. In particular, the \(G_i\)s are in 1-to-1 correspondence with non-zero \(Y_i\)s, so</p> \[Y_1^2 + Y_2^2 + ... + Y_m^2 = G_1^2 + G_2^2 + ...\] <p>We sum up to \(m\) because there are \(m\) homologous k-mers since \(m &lt; n\). But now, the <em>number</em> of \(Y_i\)s is no longer a random variable! So we can just take expectations and use linearity of expectation to get a result.</p> <h3 id="making-k-mers-independent-as-an-upper-bound">Making k-mers independent as an upper bound.</h3> <p>We’re almost there, but not quite yet. We do a trick to upper bound \(Y_1^2 + ... + Y_m^2\). The problem is that in conditions (1), (2), and (3) above, the k-mers are not independent. Precisely, the “k-mers between these two flanking k-mers” in (3) share bases with the flanking k-mers, so they’re dependent under our mutation model.</p> <p>To remove this dependence, we instead <strong>only consider k-mers that are spaced exactly k bases apart</strong>. That is, let \(K\) be a maximal set of k-mers spaced exactly \(k\) bases apart from each other. k-mers in \(K\) do not overlap at all. The bottom set of k-mers in the below picture is \(K\).</p> <p style="text-align: center;"><img src="/assets/img/K_kmer.png" alt="test" width="30%"/></p> <p><strong>Definition</strong>: Now let \(Y_i^K\) be the random variable such that \(Y_i^K = x\) if</p> <ol> <li>the k-mer starting at position \(i\) is unmutated (none of its bases are mutated) <strong>and this k-mer is in \(K\)</strong></li> <li>and the k-mer starting at position \(i + x + k\) is unmutated <strong>and this k-mer is in \(K\)</strong></li> <li>and no k-mers <strong>in \(K\)</strong> between these two flanking k-mers are unmutated (i.e. they’re all mutated).</li> </ol> <p>And we let \(Y_i^K = 0\) if any of these conditions are violated. \(Y_i^K\)s represent the gaps when only considering k-mers in \(K\).</p> <p><strong>Claim</strong>: It should be intuitively obvious that the gaps are <em>larger</em> when only considering k-mers in \(K\) instead of all k-mers, thus we get an <em>upper bound</em> by considering \(Y_i^K\)s; we prove this more rigorously in Appendix D.5.</p> <p>Thus we are left with bounding</p> \[\text{Runtime} \leq O(\mathbb{E}[Y_1^2 + ... + Y_m^2]) \leq O(\mathbb{E}[(Y_1^K)^2 + ... + (Y_m^K)^2])\] <p>Recall that \(m\) is the size of \(S'\) (modulo factors of \(k\), which are small), the smaller string. Now we can see that all except for about \(m/k\) of the \(Y_i^K\) are always \(0\) since there are only \(m/k\) k-mers in \(K\), so only \(m/k\) of the random variables satisfy condition (1) above.</p> <h3 id="calculating-expectation-of-y_ik2">Calculating expectation of \((Y_i^K)^2\).</h3> <p>Let’s compute \(\Pr(Y_i^K = x)\). I claim that assuming the k-mer at \(i\) is in \(K\),</p> \[\Pr(Y_i^K = k \cdot \ell) \leq (1-\theta)^k \cdot (1-\theta)^k \cdot (1 - (1-\theta)^k)^{\ell}\] <p>Firstly, \(Y_i^K\) can not equal non-multiples of \(k\) by our construction. The first \((1-\theta)^k\) comes from condition (1) above, and the second follows from condition (2). The probability a k-mer is unmutated is \((1-\theta)^k\) because all mutations are independent. The last term comes from the fact that if there are \(\ell \cdot k\) bases between the first and last k-mer, there are exactly \(\ell\) k-mers in between them (in \(K\)). These k-mers are independent, so the probability that all of them are mutated is \((1 - (1-\theta)^k)^\ell\). The \(\leq\) follows because the gap size can not be greater than \(m\), so the actual value would be 0 in that case.</p> <p>Now we can finally calculate what we wanted, which is \(\mathbb{E}[(Y_i^K)^2]\). This is</p> \[\mathbb{E}[(Y_i^K)^2] = \sum_{x=1}^\infty x^2 \Pr(Y_i^K = x) \leq \sum_{\ell = 1}^\infty (\ell k)^2 (1-\theta)^{2k} (1 - (1-\theta)^k)^{\ell}.\] <p>We only sum over \(\ell\), so this is</p> \[= (1-\theta)^{2k} k^2 \sum_{\ell = 1}^\infty \ell^2 (1-(1-\theta)^k)^{\ell}.\] <p>Geometric series calculus (or Mathematica) tells us that \(\sum_{i=1}^\infty i^2 x^i = \frac{x (x+1)}{(1-x)^3}\) (there’s a typo in version 1 of our paper on pg. 28; this is the correct formula). Thus plugging this in, we get</p> \[\mathbb{E}[(Y_i^K)^2] \leq (1-\theta)^{2k} k^2 \cdot O \bigg( \frac{1}{(1-\theta)^{3k}} \bigg) = O \bigg( \frac{k^2}{(1-\theta)^k} \bigg)\] <p>after remembering that \((1-\theta)^k = o(1)\) because we assumed that \(k = C \log n\) way back for some constant \(C\). Now recall there are \(m/k\) non-zero \(Y_i^K\) random variables, so our final number of gaps squared is</p> \[\mathbb{E}[(Y_1^K)^2] + ... + \mathbb{E}[(Y_m^K)^2] \leq \frac{m}{k} \cdot O \bigg( \frac{k^2}{(1-\theta)^k} \bigg) = O \bigg( m k (1-\theta)^{-k}\bigg).\] <p>After a final substitution of \(k = C \log n\), we get that</p> \[= O \bigg (m \log n (1 - \theta)^{-C \log n} \bigg) = O\bigg( m \log n n^{-C \log (1 - \theta)}\bigg)\] <p>where \(-C \log (1 - \theta) = f (\theta)\), and we’re done. In the paper, we take \(C \sim \frac{2}{1 + 2 \log(1-\theta)}\) for reasons relating to the accuracy result, so \(C\) actually depends on \(\theta\) as well.</p> <h3 id="conclusion">Conclusion</h3> <p>We showed that the runtime of extending through the gaps in the homologous chain of k-mers is \(O(m n^{f(\theta)} \log n )\), which is almost \(O(m \log n)\) when \(\theta\) is small. For example, we can explicit calculate \(f(0.05) &lt; 0.08\) when \(\theta\) is \(0.05\), so the runtime in this case this is \(O(m n^{0.08} \log n)\). \(n^{0.08}\) is actually smaller than \(\log n\) when \(n &lt; 10^{21}\), so it’s really, really small for realistic \(n\).</p> <p>Remember, I did not claim that we’re actually aligning through the homologous chain; we’re aligning through the optimal chain given by linear gap cost chaining. However, in the paper, we show that the actual runtime is dominated by this homologous chain extension runtime. This shouldn’t be too head-scratching; most of the time (especially without repeats), we expect the correct chain to be the obvious, homologous one. It’s surprisingly challenging to prove, though, and comes down to bounding <em>breaks</em>, which I won’t go into here.</p> <p>Last thought I’d like to mention: one of the punchlines of our paper involves sketching. We show that <em>this same bound holds</em> even when we subsample the k-mers. The proof for that result follows the same intuition as this proof, but it actually uses a non-independent Chernoff bound to bound the \(Y_i\)s instead, leading to some slightly different techniques.</p>]]></content><author><name></name></author><category term="jekyll"/><category term="update"/><summary type="html"><![CDATA[In our (me and Yun William Yu’s) new paper “Seed-chain-extend alignment is accurate and runs in close to O(m log n) time for similar sequences: a rigorous average-case analysis”, we give rigorous bounds on seed-chain-extend alignment using average-case analysis on random, mutating strings. This blog post is meant to be:]]></summary></entry><entry><title type="html">Average contig length times 1.6783469… is equal to N50</title><link href="https://jim-shaw-bluenote.github.io//blog/2022/average-contig-length-n50/" rel="alternate" type="text/html" title="Average contig length times 1.6783469… is equal to N50"/><published>2022-09-26T00:00:00+00:00</published><updated>2022-09-26T00:00:00+00:00</updated><id>https://jim-shaw-bluenote.github.io//blog/2022/average-contig-length-n50</id><content type="html" xml:base="https://jim-shaw-bluenote.github.io//blog/2022/average-contig-length-n50/"><![CDATA[<h3 id="probabilistic-setup-of-sequence-assembly-outputs">Probabilistic setup of sequence assembly outputs</h3> <p>When one analyzes sequence assemblies, a mysterious definition they inevitably encounter is the N50. N50 is commonly defined as the size \(L\) of the smallest contig such that summing all contigs of size \(\geq L\) gives at least half of the total sequence assembly length. The reason for using N50 intuitively is that we don’t want small contigs, of which there may be many, to skew the average contig length. See <a href="http://www.acgt.me/blog/2013/7/8/why-is-n50-used-as-an-assembly-metric.html">here</a> or the <a href="https://en.wikipedia.org/wiki/N50,_L50,_and_related_statistics">Wikipedia article</a> for more info.</p> <p>Given a set of contigs coming from an assembly, how does the N50 relate to the average contig length? This may seem like a silly question because they aren’t related in general; you can easily come up with distributions of contig sizes with fixed average length and wildly varying N50s. However, I’ll show that under some (maybe not so mild) assumptions, the average contig length and N50 are related by a surprisingly simple formula.</p> <p>Let’s make the following assumptions:</p> <ol> <li> <p>Assume that we have a fixed number of \(n\) contigs of size \(x_1,...,x_n\) and define \(\sum_{i=1}^n x_i = N\). Here \(x_1,..,x_n\) are the sizes of contigs output by an assembly, and \(N\) is our putative assembly length.</p> </li> <li> <p>Assume that \(x_i\) are random, i.i.d, and exponentially distributed \(x_i \sim Exp(\frac{1}{\lambda})\) with mean length \(\lambda.\) Contigs have continuous, random size in our setup.</p> </li> </ol> <p>I chose an exponential distribution because contig lengths tend to have a heavy tail with a few large contigs and many small ones.</p> <p>Under our probabilistic setup, the average length of the contigs is \(\mathbb{E}[x_i] = \lambda\), but <em>can we calculate what the N50 is analytically?</em></p> <h3 id="defining-n50-probabilistically">Defining N50 probabilistically</h3> <p>First, let’s define the <em>empirical N50</em> rigorously.</p> <p><em>Definition</em>: The empirical N50, \(\hat{N_{50}}\), is a random variable defined as the smallest \(x_j = \hat{N_{50}}\) where \(x_j \in \{x_1,...,x_n\}\) such that</p> \[\sum_{x_i \geq \hat{N_{50}}} x_i = \sum_{i=1}^n x_i \mathbb{1}_{x_i \geq \hat{N_{50}}} \geq \frac{N}{2}.\] <p>This is just a mathematical generalization of the original definition of the N50 if you have a bunch of samples (i.e. contigs). Now I’ll introduce a probabilistic definition of the N50.</p> <p><em>Definition</em>: The N50 is a constant \(N_{50}\) satisfying</p> \[\mathbb{E}[\sum_{i=1}^n x_i \mathbb{1}_{x_i \geq N_{50}}] = \frac{\mathbb{E}[N]}{2} = \frac{n\lambda}{2}.\] <p>The N50 is not a random variable, but a specific constant defined to satisfy an equation. Intuitively, the N50 is what we believe the value of empirical N50, \(\hat{N_{50}}\), should be when \(n\), the number of samples, gets really large. Notice that this is essentially a generalization of the probabilistic definition of the median and the empirical median. Here is a suggestive sketch of an argument that these two quantities are related.</p> <p>Notice that</p> \[\frac{N}{2} + \max_{m=1,...,n} x_m \geq \sum_{x_i \geq \hat{N_{50}}} x_i = \sum_{i=1}^n x_i \mathbb{1}_{x_i \geq \hat{N_{50}}} \geq \frac{N}{2}\] <p>follows after thinking about the definition of the empirical N50 for a bit. It turns that \(\mathbb{E}[\max_{m=1,...,n} x_m] = \sum_{i=1}^n \frac{1}{i} = O(\log n)\) is the nth harmonic number, see <a href="https://stats.stackexchange.com/questions/324274/how-to-find-the-expectation-of-the-maximum-of-independent-exponential-variables">this explanation</a>. Then taking expectations where \(\mathbb{E}[N] = n \lambda\) and dividing by \(n\), we get that</p> \[\frac{\lambda}{2} + o(1) &gt; \frac{\mathbb{E}[\sum_{i=1}^n X_i \mathbb{1}_{x_i \geq N_{50}}]}{n} \geq \frac{\lambda}{2}.\] <p>So \(\frac{\mathbb{E}[\sum_{x_i \geq \hat{N_{50}}} x_i]}{n} \rightarrow \lambda/2.\) Thus, from the definition of empirical N50,</p> \[\lim_{n \rightarrow \infty} \frac{\mathbb{E}[\sum_{i=1}^n x_i \mathbb{1}_{x_i \geq \hat{N_{50}}}]}{n} = \frac{\mathbb{E}[\sum_{i=1}^n x_i \mathbb{1}_{x_i \geq N_{50}}]}{n}.\] <p>This is the reason behind the definition, and suggests that \(\hat{N_{50}} \rightarrow N_{50}\) is some sense, probably in probability.</p> <p>I haven’t tried too hard to prove that \(\hat{N_{50}} \rightarrow N_{50}\) as n gets large. The proof should probably be similar to the <a href="https://stats.stackexchange.com/questions/72023/convergence-in-probability-of-empirical-median">proof that the empirical median converges to the true median</a>. In my simulations, it seems to be the case that \(\mathbb{E}[\hat{N_{50}}] \rightarrow N_{50}\), so let’s just assume that these quantities are related for the rest of the post. Maybe someone else can take a stab at it; let me know how it goes!</p> <h3 id="calculating-the-n50">Calculating the N50</h3> <p>Let’s define \(\sigma = N_{50}\) for the rest of the post, for ease of notation. We can actually calculate the N50 as follows. Because \(\sigma\) is a constant, \(x_i \mathbb{1}_{x_i \geq \sigma}\) are identically distributed. Thus to find the N50, we just have to solve the equation</p> \[\mathbb{E}[\sum_{i=1}^n x_i \mathbb{1}_{x_i \geq \sigma}] = n \int_{z \geq \sigma}^\infty z \frac{1}{\lambda} e^{-\frac{z}{\lambda}} dz = \frac{n \lambda }{2}\] <p>for \(\sigma\). The \(n\) cancels out nicely, and evaluating the integral yields the equation</p> \[(\sigma + \lambda) = \frac{\lambda}{2} e^{\frac{\sigma}{\lambda}}\] <p>This is actually not a trivial equation to solve. You can try isolating for \(\sigma\) and you’ll see that it doesn’t work so easily. Here’s how you solve it:</p> <p>First, we make the substitution \(y = \sigma + \lambda\). This simplifies it to</p> \[y = \frac{\lambda}{2} e^{\frac{y}{\lambda} - 1}\] <p>and after another round of simplifications this becomes</p> \[2e = \frac{\lambda}{y}e^{\frac{y}{\lambda}} \iff \frac{1}{2e} = \frac{y}{\lambda}e^{-\frac{y}{\lambda}}\] <p>assuming \(y \not = 0\), of course. Finally, we let \(y \mapsto -y\) to get</p> \[\frac{-1}{2e} = \frac{y}{\lambda}e^{\frac{y}{\lambda}}.\] <p>At this point, you’re probably either wondering about why we went through this seemingly random derivation, or you can see exactly what the next step is. We make one last substituion \(w = \frac{y}{\lambda}\) to get</p> \[\frac{-1}{2e} = w e^{w}.\] <p>This equation still isn’t easy to solve, but luckily, this type of equation pops ups all the time and has been standardized. The \(w\) that gives the solution is defined as the output of the <a href="https://en.wikipedia.org/wiki/Lambert_W_function">Lambert W function</a>, \(W(z)\), evaluated at \(z = \frac{-1}{2e}\). \(W(z)\) is a multivalued function when \(z\) is complex, and for \(z\) real, it has two solutions when \(\frac{-1}{e} \leq z &lt; 0\), as is our case. To make \(\sigma &gt; 0\), we need the function evaluated on the branch defined as \(W_{-1}(z)\). It turns out that \(W_{-1}(\frac{-1}{2e}) = -2.6783469...\). Remembering that \(w = \frac{-y}{\lambda} = \frac{-\sigma}{\lambda} - 1\), we get that finally</p> \[\sigma / \lambda = 1.6783469...\] <p><strong>So \(\sigma\), the N50, is a constant multiple of the average contig length!</strong></p> <p>It seems that this value \(1.6783469...\) appears elsewhere in statistical theory. A quick google search yields the article “Asymptotic Inversion of Incomplete Gamma Functions” by N.M Temme (1992) as an example where this value appears.</p> <p>So under our exponential model, along with our numerous assumptions, the N50 and the average contig length is always related just by a constant multiple, no matter what the average length is.</p> <h3 id="does-this-actually-work-in-practice">Does this actually work in practice?</h3> <p>To apply this theory, we can’t look at genomes that have been meticulously refined, as they no longer have exponentially distributed contig lengths.</p> <p>To investigate a more raw dataset, I took a set of MAGs, i.e. metagenome assembled genomes from the study <a href="https://www.nature.com/articles/s41564-017-0012-7">here</a> and uploaded <a href="http://enve-omics.ce.gatech.edu/data/fastani">here</a>. I calculated the N50s for a bunch of the genome assemblies, and here is the resulting histogram.</p> <p><img src="/assets/img/empirical_n50_avg_len_ratio.png" alt="Actual N50/Average length distribution"/></p> <p>It turns out that the predicted N50/Average length ratio is about 3.4 percent off from the empirical value. Considering how often and how badly theoretical models fail in bioinformatics, I’ll consider this a win :).</p>]]></content><author><name></name></author><category term="jekyll"/><category term="update"/><summary type="html"><![CDATA[Probabilistic setup of sequence assembly outputs]]></summary></entry></feed>